"use strict";(self.webpackChunklh_site=self.webpackChunklh_site||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"challenge-of-microservices","metadata":{"permalink":"/blog/challenge-of-microservices","source":"@site/blog/2024-08-27-challenges-of-microservices.md","title":"The Challenge of Microservices","description":"Microservices are often necessary, but unfortunately they bring with them some baggage.","date":"2024-08-27T00:00:00.000Z","tags":[{"inline":false,"label":"Technical Analysis","permalink":"/blog/tags/analysis/","description":"Analysis of the current and future state of Technical Architecture."}],"readingTime":8.94,"hasTruncateMarker":true,"authors":[{"name":"Colt McNealy","title":"Managing Member of the LLC","description":"Colt is the founder of LittleHorse Enterprises and the original author of the LittleHorse Orchestrator. He\'s a passionate Apache Kafka fan and loves hockey, golf, piano, cooking, and Taekwondo.","page":{"permalink":"/blog/authors/coltmcnealy"},"socials":{"github":"https://github.com/coltmcnealy-lh","linkedin":"https://www.linkedin.com/in/colt-mcnealy-900b7a148/","x":"https://x.com/coltmcnealy"},"imageURL":"https://avatars.githubusercontent.com/u/100447728","key":"coltmcnealy"}],"frontMatter":{"slug":"challenge-of-microservices","title":"The Challenge of Microservices","authors":["coltmcnealy"],"tags":["analysis"]},"unlisted":false,"nextItem":{"title":"The Promise of Microservices","permalink":"/blog/promise-of-microservices"}},"content":"Microservices are often necessary, but unfortunately they bring with them some baggage. \x3c!-- truncate --\x3e\\n\\n:::info\\nThis is the second part of a 3-part blog series:\\n\\n1. [The Promise of Microservices](./2024-08-22-promise-of-microservices.md)\\n2. **[This Post]** The Challenge with Microservices\\n3. **[Coming Soon]** Workflow and Microservices: A Match Made in Heaven\\n:::\\n\\nLast week, I [blogged](./2024-08-22-promise-of-microservices.md) about the problems that microservices solve, and why they are not only beneficial but necessary in some cases (a good bellwether is the size of your engineering team: beyond 1 or 2 dozen engineers, you will probably start to feel some problems that can be solved with microservices).\\n\\nWhen done correctly, microservices remove several bottlenecks to scaling your business. However, even well-architected microservices bring significant _accidental complexity_.\\n\\nIn particular, microservices are:\\n\\n1. Harder to **observe** and debug.\\n2. Harder to make **reliable** in the case of infrastructure or sofware failures.\\n3. More complex to **maintain** and evolve with changing business practices.\\n\\nIn this article we will explore how the above problems arise from two key facts:\\n* Microservices are **distributed**.\\n* Microservices are **choreographed without a leader**.\\n\\n:::note\\nMicroservices bring with them additional challenges around operationalization and deployment. However, those challenges are out-of-scope for this blog post as we instead choose to focus on the challenges faced by _application development teams_ rather than operations teams.\\n:::\\n\\n## The Nature of Microservices\\n\\nAs I described in [last week\'s blog](./2024-08-22-promise-of-microservices.md):\\n\\n> The term \\"microservices\\" refers to a software architecture wherein an enterprise application comprises a collection of small, loosely coupled, and independently deployable services (these small services are called \\"microservices\\" in contrast to larger monoliths). Each microservice focuses on a specific business capability and communicates with other services over a network, typically through API\'s, streaming platforms, or message queues.\\n\\nCrucially, a single microservice implements technical logic for a specific domain, or bounded context, within the larger company. In contrast, a comprehensive business process requires interacting with technology and people across _many_ business domains. The classic example of microservices architecture, e-commerce checkout, involves at least _shipping_, _billing_, _notifications_, _inventory_, and _orders_.\\n\\nIn the rest of this blog post we will examine microservices through the the lense of e-commerce checkout flow. To start with a simple use-case, the logical flow we will consider is:\\n\\n1. When an order is placed, we create a record in a database in the `orders` service.\\n2. We then reserve inventory (and ensure that the item is in stock) in the `inventory` service.\\n3. We charge the customer using the `payments` service.\\n4. Next, we ship the item using the `shipping` service.\\n5. Finally, the `notifications` service notifies the customer that the parcel is on its way.\\n\\n![Simple e-commerce workflow](./2024-08-27-simple-checkout.png)\\n\\n### Microservices are Distributed\\n\\nRecall that each service (in the workflow diagram above, each box) is its own deployable artifact. That means that the happy-path business process described above will involve five different software systems from start-to-finish.\\n\\nIn the above workflow diagram, each arrow can be accurately interpreted in two ways:\\n1. The logical flow of the business process.\\n2. The physical flow of information between microservices, either through network RPC calls or through a message broker like Apache Kafka.\\n\\nGuess what! This means we have a distributed system by definition. As Splunk [writes in a blog post](https://www.splunk.com/en_us/blog/learn/distributed-systems.html):\\n> A distributed system is simply any environment where multiple computers or devices are working on a variety of tasks and components, all spread across a network.\\n\\nYou need to look no further than the [Fallacies of Distributed Computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing) (written by Sun Microsystems Fellow L. Peter Deutsch in 1994) to see that this means that microservices are no easy task.\\n\\n### Microservices are Leaderless\\n\\nAs we\'ve seen already, any microservice-based application is a distributed system. Some distributed systems have the concept of a _leader_, which is a special node in the system that has special responsibilities.\\n\\n:::info\\nApache Kafka is my favorite distributed system. In Apache Kafka, the _Controller_ is a special Kafka server that is responsible for deciding which partition replicas are hosted on (and led by) which brokers. If the broker who was in charge of a partition goes down, then the Controller chooses a new broker from the ISR to take its place.\\n\\nTherefore, the _Controller_ in Apache Kafka can be thought of as a _leader_.\\n:::\\n\\nWhile systems like Apache Kafka have clear leaders (for example, the _Controller_ may re-assign partition leadership if the cluster becomes too imbalanced), in a microservice-based system there is no central leader to ensure that the chips fall correctly. This is by necessity, because the separation of development concerns and lifecycles across microservices means that microservices cannot and do not have leaders.\\n\\nYou can think of our e-commerce microservice flow as a line of dominoes falling. Once the process starts, no one entity is responsible for ensuring its completion. The business workflow moves from `orders` to `inventory` to `payments` and so on. If `payments` fails for some reason (perhaps a network outage makes the Stripe API unavailable), then it\'s quite possible that the `shipping` service never finds out about the workflow.\\n\\nHowever, in real life such outcomes are not acceptable. This means that every single player in the system must:\\n\\n1. Have built-in reliability mechanisms.\\n2. Understand the preceding and subsequent steps of the business process to route traffic.\\n\\nImplementing the above slows down development, more tightly couples one services to another, increases dependencies, and makes your microservice architecture much more heavyweight.\\n\\n## The Challenges\\n\\nSo far, we have established that there are many players involved in a business process, yet there\'s no one orchestrator involved in ensuring that an ordered item is delievered to the the correct address. This yields three problems:\\n\\n1. **Reliability** in the face of infrastructure failures.\\n2. **Observability** to enable system optimization and debugging.\\n3. **Coupling** of microservices to each other makes it hard to modify the system in response to new business requirements.\\n\\n### Reliability and Correctness\\n\\nProcessing orders is a mission-critical use-case. This means that orders should always complete and never be dropped (for example, we should not charge the customer\'s credit card and not ship the product to them).\\n\\nHowever, asynchronous processing such as that which I outlined above is prone to failures. For example, if you chain microservices together with direct RPC calls, a single network partition can cause an order to get stuck. Even with a reliable message broker such as Apache Kafka or AWS SQS sitting between your microservices, a write to the message broker could fail _after_ the payment went through, still resulting in a stuck order.\\n\\nJust as communication _between_ microservices can fail, the actions performed _by_each microservice can also fail. In many cases actions performed by a microservice depend upon failure-prone external systems and API\'s. If the Stripe API is down, or if the credit card is invalid, we can\'t just stop processing the order there! We must notify the customer of what went wrong and also release the inventory that we reserved.\\n\\nThis means that microservice developers spend countless hours building out infrastructure to support:\\n* Retries\\n* Dead-Letter Queues\\n* Rate-limiting\\n* Timeouts\\n* Transactional Outbox pattern\\n* SAGA Pattern\\n\\nBack to the domino analogy, if one domino misses the next, the entire chain just stops.\\n\\n### Observability\\n\\nThe second problem with microservices is that once a process instance has started (i.e. the dominoes are falling), it is very difficult to observe what happens between steps 2 through 10. This means that multi-step processes with performance issues are hard to optimize, as there are many microservices which could be the bottleneck and it\'s hard to know which. Even worse, when a customer complains about a \\"stuck order,\\" it is difficult to find the point of failure.\\n\\nAs a result, microservice engineers spend time and money:\\n* Slogging through logs on DataDog\\n* Implementing complex distributed tracing such as Zipkin, Jaeger, or Kiali\\n* Saving the state of each process instance (in our case, the `order`) in a DB just for visibility purposes at every step\\n* Coordinating with other teams to manually understand and debug workflows.\\n\\n### Microservice Coupling\\n\\nLastly, because microservices are leaderless, each player in the end-to-end process must have hard-coded integrations with the preceding and subsequent steps. This results in:\\n\\n* **Process coupling**, wherein changing a business process results in significant code updates to rewire the message queues or RPC calls between two steps.\\n* **Schema coupling**, wherein different microservices have strong dependencies on each others\' schemas.\\n\\nMicroservices come with the promise of loose coupling; however, the unfortunate reality is that this is often not the case. As a result, teams often do have to coordinate with each other during deployments.\\n\\nTo see an example of the complexity introduced by coupling of microservices, let\'s consider what happens to our e-commerce checkout workflow when we add a few edge cases to make it more realistic:\\n\\n1. If the credit card is invalid, we request the customer to provide a new one, wait for two days, and either complete or cancel the order.\\n2. If the item is out of stock, we notify the customer who elects either to wait or cancel the order.\\n\\n![Complex Checkout Architecture](./2024-08-27-complex-checkout.png)\\n\\nIn the above diagram, each arrow represents the flow of the business process _and_ information. Each microservice must have custom logic which sends information to the right place. In essence, while we _intended_ to have modular microservices that understand only their own Bounded Context, what we have is tightly-coupled systems which must understand pretty much the entire business workflow.\\n\\nTherefore, when business requirements change, unrelated microservices end up having to change their internal implementation as well.\\n\\n## Looking Forward\\n\\nMicroservices have clear and proven benefits, and are often not just advantageous but _necessary_ in some cases. However, as we discussed today, those benefits do not come without a cost. Because microservices are inherently distributed systems, challenges such as reliability, observability, and coordination are exacerbated.\\n\\nWithout spoiling the punchline of the next blog post, these challenges are why I started LittleHorse almost three years ago. Stay tuned for a description of how a _workflow orchestrator_ can alleviate a good portion of the headaches that come along with microservices.\\n\\n### Business Analytics\\n\\nAstute readers may notice that when discussing the e-commerce checkout example, we didn\'t discuss the problem of _analytics._ We focused exclusively on online transaction processing, or ensuring that the orders are properly fulfilled and processed. However, no attention was paid to business analytics to optimize future sales!\\n\\nThis area is yet another challenge. The LittleHorse Council is working on a major feature (an output Kafka Topic with records for anytime something _interesting_ happens inside a `WfRun`) for the LH Server that will address this. Don\'t worry, we\'ll blog about it soon :wink:."},{"id":"promise-of-microservices","metadata":{"permalink":"/blog/promise-of-microservices","source":"@site/blog/2024-08-22-promise-of-microservices.md","title":"The Promise of Microservices","description":"If microservices add so much complexity, why bother with the hassle?","date":"2024-08-22T00:00:00.000Z","tags":[{"inline":false,"label":"Technical Analysis","permalink":"/blog/tags/analysis/","description":"Analysis of the current and future state of Technical Architecture."}],"readingTime":7.1,"hasTruncateMarker":true,"authors":[{"name":"Colt McNealy","title":"Managing Member of the LLC","description":"Colt is the founder of LittleHorse Enterprises and the original author of the LittleHorse Orchestrator. He\'s a passionate Apache Kafka fan and loves hockey, golf, piano, cooking, and Taekwondo.","page":{"permalink":"/blog/authors/coltmcnealy"},"socials":{"github":"https://github.com/coltmcnealy-lh","linkedin":"https://www.linkedin.com/in/colt-mcnealy-900b7a148/","x":"https://x.com/coltmcnealy"},"imageURL":"https://avatars.githubusercontent.com/u/100447728","key":"coltmcnealy"}],"frontMatter":{"slug":"promise-of-microservices","title":"The Promise of Microservices","authors":["coltmcnealy"],"tags":["analysis"]},"unlisted":false,"prevItem":{"title":"The Challenge of Microservices","permalink":"/blog/challenge-of-microservices"},"nextItem":{"title":"Releasing 0.10","permalink":"/blog/littlehorse-0.10-release"}},"content":"If microservices add so much complexity, why bother with the hassle? \x3c!-- truncate --\x3e\\n\\n:::info\\nThis is the first part of a 3-part blog series:\\n\\n1. **[This Post]** The Promise of Microservices\\n2. [The Challenge with Microservices](./2024-08-27-challenges-of-microservices.md)\\n3. **[Coming Soon]** Workflow and Microservices: A Match Made in Heaven\\n:::\\n\\n\\nWe\'ve all _heard of_ microservices, but unless you\'ve read copious amounts of Sam Newman and Adam Bellemare\'s writings, you might be wondering whether, when, and why you should adopt them. In this blog post, we will examine the halcyon land promised by microservices.\\n\\nMicroservices have been [deployed widely](https://www.simform.com/blog/microservices-examples/) across many large enterprises, most notably Netflix, Uber, Shopify, PayPal, and others. As we will discover throughout this blog series, a microservice architecture is mandatory once you reach a certain size of company, and it\'s probably overkill for a 12-person startup. The gray area inbetween is the interesting part!\\n\\n## What are Microservices?\\n\\nThe term \\"microservices\\" refers to a software architecture wherein an enterprise application comprises a collection of small, loosely coupled, and independently deployable services (these small services are called \\"microservices\\" in contrast to larger monoliths). Each microservice focuses on a specific business capability and communicates with other services over a network, typically through API\'s, streaming platforms, or message queues.\\n\\nIn practice, this means that a user interaction with an application (such as placing an order) might trigger actions that occur in _many_ small, independently-deployed software systems, such as:\\n\\n* A Notification service\\n* An Inventory Management service\\n* A Payments service\\n* An Order History service\\n\\nFrom the user (client) perspective, one request is made (generally through a Load Balancer, API Gateway, or Ingress Controller) but that request may ping-pong between multiple back-end services and may also result in future actions being scheduled asynchronously:\\n\\n![Microservices Architecture](./2024-08-22-microservices-arch.png)\\n\\nIn contrast to microservices, a _monolithic_ architecture would serve the entire \\"place order\\" request on a single deployable artifact:\\n\\n![Monolithic Architecture](./2024-08-22-monolith-arch.png)\\n\\nIn Domain Driven Design, accidental complexity refers to the unintentional complexity that you introduced to your architecture (deployments, service interactions, third-party dependencies, etc.). Rule #1 of maintaining software systems is to avoid introducing accidental complexity as much as possible.\\n\\nSimply by looking at the visuals above, microservices add a significant dose of accidental complexity to your architecture (more on this in next week\'s post!). Given that, what benefits would make up for the extra complexity introduced by microservices?\\n\\n## Why Now?\\n\\nI would be first to admit that microservices bring with them a series of headaches around cost, observability, maintenance, and ease of evolution (otherwise, I would not have founded LittleHorse Enterprises!). However, microservice architecture plays a vital role in addressing two critical trends reshaping the software development landscape today:\\n\\n* Increased digitization of companies in all business sectors (accelerated by the rise of AI).\\n* Elasticity of cloud computing.\\n\\n### Increased Digitization\\n\\nThe level of digitization expected of businesses in order to compete in the modern market has drastically increased: IT teams must build software that interfaces with an ever-expanding list of external API\'s, legacy systems, user interfaces, internal tools, and SaaS providers.\\n\\nFor example: in the early 2000\'s, it was perfectly acceptable (even _expected_) for a passenger to book airline tickets over the telephone or through a travel agency. However, such an experience would be unheard of today and would immediately hobble an airline who provided such poor digital services.\\n\\nIn addition to using automation to provide better customer services, companies are generating, processing, and analyzing massive amounts of data. For example, grocery stores with razor-thin margins analyze seasonal consumption patterns in order to optimize inventory and prevent costly food waste.\\n\\nThese trends have coincided with (or _caused_, I would argue) a proliferation in the number of 1) software developers, and 2) software tools and API\'s found within companies in all industries, leading to two new problems:\\n\\n1. Allowing large teams of software developers to productively work on an enterprise application in parallel (without stepping on each others\' toes).\\n2. Ensuring that business requirements are effectively communicated to the entire (larger) software engineering team.\\n\\n### Cloud Elasticity\\n\\nAs the importance and quantity of digital software systems exploded over the last two decades, so has the availability of nearly-infinite compute power delivered through cloud infrastructure providers such as AWS.\\n\\nThe promise of _elasticity_, or the ability to quickly spin compute resources up or down according to load and only pay for what you use, is unique to the cloud: for on-prem datacenters, spinning up new compute means buying new machines from Sun Microsystems (hopefully not Microsoft!), and scaling down compute means trying to sell them off on the secondary market. (Ask my father about how that went for a lot of people in 2001.)\\n\\nBeyond scaling up and down, elasticity enables different deployment patterns that did not exist before. Whereas pre-cloud enterprises had dedicated and centralized data-center teams who were in charge of running applications, the accessibility of cloud computing gave rise to the DevOps movement. This has empowered smaller teams of software developers to take on the task of transferring software from \\"it works on my laptop!\\" to \\"it\'s now deployed in production!\\"\\n\\n## Why Microservices?\\n\\nDespite the extra complexity it brings, the microservice architecture can more than pay for itself by ensuring organizational alignment and allowing enterprise architectures to take full advantage of the cloud\'s elasticity.\\n\\n### Organizational Alignment\\n\\nAs discussed earlier, the business problems that software engineering organizations must solve today dwarf those that were solved in the 1990\'s, and so do the software engineering teams that tackle those problems.\\n\\n:::note\\nI am not belittling the engineers of the 90\'s; the problems they solved were arguably _much harder_ than the problems we face today, and there were fewer engineers to face those problems. However, it is a fact that users expect more digital-native experiences today than they did twenty years ago.\\n:::\\n\\nBy breaking applications into smaller services, we can accomplish several important things:\\n* Break up our software engineering team into smaller teams which are each responsible for individual microservices.\\n* Allow different components of a system to be developed with separate tech stacks and released independently.\\n\\nEngineering teams of over a few dozen engineers working on the same deployable piece of software is a recipe for inefficiency. Merge conflicts, arguments over tech stack, slow \\"release trains,\\" and excessive intra-team coordination are just a few problems that arise. However, by breaking your application into smaller microservices, you can also break up your engineering organization into smaller, more efficient teams each in charge of a small number (prefably one!) of microservices.\\n\\nAs an added benefit, properly-designed microservice architectures can follow the principles of Domain Driven Design. Ideally, a single microservice corresponds to a _Bounded Context_ inside the business. This enables a small piece of the technical platform (a microservice) to be managed by a small team of software engineers, who collaborate closely with subject-matter experts and business stakeholders within a very specific domain of the business. Such close collaboration can foster better alignment between business goals and the software produced by engineering teams.\\n\\n### Moving Faster\\n\\nMicroservices can allow developers to move faster by enabling continuous delivery and independent deployment of services. In a monolithic architecture, releasing a new feature or fixing a bug typically requires redeploying the entire application. Since microservices allow smaller pieces of your application to be deployed independently, engineering teams can iterate faster and deliver incremental value to business stakeholders.\\n\\nThese positive effects are amplified by the advent of cloud computing. Since deploying a new application no longer requires buying a physical machine and plugging it into your datacenter but rather just applying a new `Deployment` and `Service` on a Kubernetes cluster, it is now truly feasible for small teams of software engineers to own their application stack from laptop-to-production (obviously, within the guardrails set by the central platform team). Furthermore, cloud computing is a pay-as-you-go (and often even pay-for-what-you-use) expense rather than an up-front cost. Therefore, the dollar cost of infrastructure required to support microservices is much lower today than it would have been before the advent of cloud computing and kubernetes.\\n\\n## Conclusion\\n\\nThe microservice architecture is not just a Twitter-driven buzzword but rather a way of designing system that has several real advantages. For most organizations with over two dozen software engineers, building applications with microservices is not an option but rather a _necessity_. However, those advantages come with a cost.\\n\\nWe will discuss those challenges in next week\'s blog post...in the meantime, though, join our [Community Slack](https://launchpass.com/littlehorsecommunity) to get the latest updates!"},{"id":"littlehorse-0.10-release","metadata":{"permalink":"/blog/littlehorse-0.10-release","source":"@site/blog/2024-07-12-0.10-release.md","title":"Releasing 0.10","description":"TBD","date":"2024-07-12T00:00:00.000Z","tags":[{"inline":false,"label":"LittleHorse Orchestrator","permalink":"/blog/tags/littlehorse/","description":"Information about the LittleHorse Orchestrator."},{"inline":false,"label":"LittleHorse Releases","permalink":"/blog/tags/release/","description":"Release blogs for LittleHorse Orchestrator."}],"readingTime":2.005,"hasTruncateMarker":true,"authors":[{"name":"The LittleHorse Council","title":"The Council of LittleHorse Maintainers","description":"LittleHorse Orchestrator is maintained by LittleHorse Enterprises LLC and available under the SSPL license. The LittleHorse Council is the group of engineers inside LittleHorse Enterprises LLC who are responsible for the stewardship of the open-source Orchestrator project and charged with looking out for the best interests of the LH Community.","url":"https://littlehorse.io","page":{"permalink":"/blog/authors/lh-council"},"socials":{"github":"https://github.com/littlehorse-enterprises","linkedin":"https://www.linkedin.com/company/littlehorse"},"imageURL":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","key":"lh_council"}],"frontMatter":{"title":"Releasing 0.10","description":"TBD","slug":"littlehorse-0.10-release","authors":["lh_council"],"tags":["littlehorse","release"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"The Promise of Microservices","permalink":"/blog/promise-of-microservices"},"nextItem":{"title":"Releasing 0.9","permalink":"/blog/littlehorse-0.9-release"}},"content":"The `0.10` release brings with it significant performance and reliability improvements. \x3c!-- truncate --\x3e\\n\\n## New Features\\n\\n### `lhctl` Binaries and Release Notes\\n\\nThe `0.10.0` release comes with a new [Release Page](https://github.com/littlehorse-enterprises/littlehorse/releases), including `lhctl` binaries built for ARM, Intel, and Windows.\\n\\n### Reliability during Rebalances\\n\\nPR [#872](https://github.com/littlehorse-enterprises/littlehorse/pull/872) improves the reliability of LittleHorse during Kafka Streams rebalances. Previously, if a write request (eg. `rpc RunWf`) was received just before a rebalance, certain requests would \\"time out\\" from the client perspective and return a `DEADLINE_EXCEEDED` grpc error despite being properly accepted and processed by the server. This PR fixes that issue by redirecting the internal `rpc WaitForCommand` to the new destination for that command.\\n\\n### Rescue Failed Workflows\\n\\nPR [#883](https://github.com/littlehorse-enterprises/littlehorse/pull/883) allows users to restart failed `WfRun`\'s via the `lhctl rescue` command. This is similar to allowing a user to execute mutating SQL queries via a CLI like `psql`.\\n\\nWith this feature, a user can fix a buggy Task Worker implementation and then restart a failed `WfRun` and get it to execute the failed `TaskRun` again via:\\n\\n```\\nlhctl rescue <wfRunId> <threadRunNumber>\\n```\\n\\n### mTLS Principals\\n\\nPreviously, only listeners of the type `OAUTH` supported `Principal`s. The `Principal` ID was determined by the OAuth Client ID or User Id. Release `0.10` introduces the ability to infer a `Principal` on an `MTLS` listener, where the `Principal` ID comes from the Common Name on the client certificate.\\n\\nPR [#874](https://github.com/littlehorse-enterprises/littlehorse/pull/874) by one of our newer team members, [Jacob Snarr](https://github.com/snarr), introduced this feature, enabling users that standardize on SSL authentication to continue using that pattern with Littlehorse.\\n\\n### Dashboard Enhancements\\n\\nThe `0.10` release includes multiple enhancements to the Admin Dashboard, including:\\n\\n* Ability to search for `WfRun`\'s by their variables.\\n* Improved `WfRun` search.\\n* Fixed display of `TaskRun`s with the `EXCEPTION` and `ERROR` status.\\n* Showing `VariableMutation`s on the `Edge` in the dashboard.\\n\\n## What\'s Next?\\n\\nWe will need one more minor release before finally releasing `1.0`. We need the following:\\n\\n* Upgrade `org.apache.kafka:kafka-streams` to `3.8.0` to address several critical reliability bugs (we are waiting for the official release).\\n* Conduct new load tests and soak tests against the new version of Kafka Streams.\\n* Review our Go and Python SDK\'s in-depth to ensure proper semantics.\\n\\nAfter that, we will be ready to commit to the backwards compatibility guarantees required by [Semantic Versioning](https://semver.org). We will also release a blog post with our planned release schedule and support schedule."},{"id":"littlehorse-0.9-release","metadata":{"permalink":"/blog/littlehorse-0.9-release","source":"@site/blog/2024-06-24-0.9.2-release.md","title":"Releasing 0.9","description":"Revamping the LittleHorse Dashboard","date":"2024-06-24T00:00:00.000Z","tags":[{"inline":false,"label":"LittleHorse Orchestrator","permalink":"/blog/tags/littlehorse/","description":"Information about the LittleHorse Orchestrator."},{"inline":false,"label":"LittleHorse Releases","permalink":"/blog/tags/release/","description":"Release blogs for LittleHorse Orchestrator."}],"readingTime":2.35,"hasTruncateMarker":true,"authors":[{"name":"The LittleHorse Council","title":"The Council of LittleHorse Maintainers","description":"LittleHorse Orchestrator is maintained by LittleHorse Enterprises LLC and available under the SSPL license. The LittleHorse Council is the group of engineers inside LittleHorse Enterprises LLC who are responsible for the stewardship of the open-source Orchestrator project and charged with looking out for the best interests of the LH Community.","url":"https://littlehorse.io","page":{"permalink":"/blog/authors/lh-council"},"socials":{"github":"https://github.com/littlehorse-enterprises","linkedin":"https://www.linkedin.com/company/littlehorse"},"imageURL":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","key":"lh_council"}],"frontMatter":{"title":"Releasing 0.9","description":"Revamping the LittleHorse Dashboard","slug":"littlehorse-0.9-release","authors":["lh_council"],"tags":["littlehorse","release"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Releasing 0.10","permalink":"/blog/littlehorse-0.10-release"},"nextItem":{"title":"Releasing 0.8","permalink":"/blog/littlehorse-0.8-release"}},"content":"The `0.9.2` release is now availble and ready for use. \x3c!-- truncate --\x3e The `0.9.x` releases focused mainly on:\\n\\n* Improving the user experience on the LittleHorse Dashboard\\n* Improving the reliability of the LH Server in the face of rebalances and failures.\\n\\n## New Features\\n\\nWhile the majority of the improvements in the `0.9` release revolve around performance and stability, several of them are highly visible to the user (especially the new dashboard!).\\n\\n### Dashboard Rewrite\\n\\nWith help from [Nelson Jumbo](https://github.com/diablouma), LittleHorse Knight [Mija\xedl Rond\xf3n](https://github.com/mijailrondon) rewrote and revamped our administrative dashboard. It now inclues new features such as:\\n\\n* User Task Detail page\\n* Improved details on `TaskRun` progress\\n* Improved details on `WfRun` progress\\n* A plethora of small bug fixes.\\n\\n### Internal Task Queue Optimizations\\n\\nDeep in the internals of the LittleHorse Server, we implement a Task Queue mechanism to store `ScheduledTask`s before they\'re dispatched to the Task Worker clients. This release included many improvements to stability of the Task Queues.\\n\\nMost importantly, our Grumpy Maintainer (Eduwer Camacaro) put a cap on the memory consumption of a single `TaskDef`. Prior to this release, it was possible for poorly-behaved clients to cause an OOM on the server by running millions of workflows which use a `TaskDef` but not executing the resulting `TaskRun`s. This would cause an un-bounded buildup of `ScheduledTask`s in memory until the server crashed.\\n\\nAfter the `0.9` release, any more than 1,000 `ScheduledTask`s for a certain `TaskDef` are not loaded into memory but left on disk.\\n\\n### Principal Deletion\\n\\nThe `0.9` release includes the ability to delete a `Principal`. The `rpc DeletePrincipal` is smart enough to ensure that there is always at least one Admin `Principal` to prevent a user from locking themselves out of the cluster.\\n\\n### `PollThread` in Java Task Worker\\n\\nWe refactored the internal implementation of the Java Task Worker so that, for each LH Server in the cluster, the Task Worker creates a single `PollThread` object which is responsible for polling and executing `TaskRun`s. The `PollThread`s now poll in parallel, drastically increasing the throughput of a single Java Task Worker.\\n\\nThe `PollThread` was introduced in [#796](https://github.com/littlehorse-enterprises/littlehorse/pull/796).\\n\\n## What\'s Next\\n\\nOur wire protocol (the GRPC API) is quite stable; there have been no major breaking changes since we introduced the alpha version of Multi-Tenancy in `0.7`. We are diligently proceeding through soak tests, load tests, and chaos tests with our server and we have found and addressed several issues.\\n\\nWe continue to look foward to the `1.0` release, and we will reach that milestone once:\\n\\n* We are satisfied with results of load tests and soak tests.\\n* We have had language experts review each of our three main SDK\'s (Java, Go, Python) and we have addressed any change requests.\\n* We approach a year without any breaking changes to our wire protocol."},{"id":"littlehorse-0.8-release","metadata":{"permalink":"/blog/littlehorse-0.8-release","source":"@site/blog/2024-03-26-0.8.1-release.md","title":"Releasing 0.8","description":"Hardening Security","date":"2024-03-26T00:00:00.000Z","tags":[{"inline":false,"label":"LittleHorse Orchestrator","permalink":"/blog/tags/littlehorse/","description":"Information about the LittleHorse Orchestrator."},{"inline":false,"label":"LittleHorse Releases","permalink":"/blog/tags/release/","description":"Release blogs for LittleHorse Orchestrator."}],"readingTime":3.955,"hasTruncateMarker":true,"authors":[{"name":"The LittleHorse Council","title":"The Council of LittleHorse Maintainers","description":"LittleHorse Orchestrator is maintained by LittleHorse Enterprises LLC and available under the SSPL license. The LittleHorse Council is the group of engineers inside LittleHorse Enterprises LLC who are responsible for the stewardship of the open-source Orchestrator project and charged with looking out for the best interests of the LH Community.","url":"https://littlehorse.io","page":{"permalink":"/blog/authors/lh-council"},"socials":{"github":"https://github.com/littlehorse-enterprises","linkedin":"https://www.linkedin.com/company/littlehorse"},"imageURL":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","key":"lh_council"}],"frontMatter":{"title":"Releasing 0.8","description":"Hardening Security","slug":"littlehorse-0.8-release","authors":["lh_council"],"tags":["littlehorse","release"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Releasing 0.9","permalink":"/blog/littlehorse-0.9-release"},"nextItem":{"title":"Releasing 0.7","permalink":"/blog/littlehorse-0.7-release"}},"content":"The `0.8` release of LittleHorse is out! This pre-1.0 release contains many new features, security enhancements, and performance improvements.\\n\\n\x3c!-- truncate --\x3e\\n\\n## New Features\\n\\nNew features in this release cover some edge-cases in workflow development which came up from some initial pilots and internal usage of the platform.\\n\\n### Dynamic Task Execution\\n\\nBefore this release, a `TaskNode` had a hard-coded reference to a `TaskDef`. This means that every single `WfRun` that reaches the same `Node` in a `WfSpec` ends up executing the same `TaskDef`.\\n\\nHowever, in LittleHorse Enterprises LLC\'s upcoming Control Plane project (a system for dynamically provisioning LittleHorse clusters as a SaaS service), we anticipate a special use-case (which we will blog about this upcoming fall) wherein we need to _choose_ which `TaskDef` is executed dynamically at runtime.\\n\\nSpecifically, depending on an input variable to a `WfRun` (in this case, the `data-plane-id` variable), we need to execute a different `TaskDef` so that the `TaskRun` is executed by a speciific Task Worker in a specific location. We will blog about that use-case later.\\n\\n### Per-Thread Failure Handlers\\n\\nSince the `0.1.0` release of LittleHorse it has been possible to put a `FailureHandler` on any `Node`, such that if the `NodeRun` fails, then a Failure Handler thread is \\n\\n### Content in `EXCEPTION`s\\n\\n- #714\\n\\n### Multi-Tenancy Improvements\\n\\nMulti-Tenancy has been quietly under development in the LittleHorse Server since the `0.6.0` release introduced a breaking change to allow for it last October. The `0.8` release continues to progress towards making Multi-Tenancy generally-available.\\n\\nThis release includes two new major features for Multi-Tenancy:\\n\\n1. Allowing Python and Go clients to set the `tenant-id` header using `LHC_TENANT_id` ([#704](https://github.com/littlehorse-enterprises/littlehorse/pull/704))\\n2. Allowing administrative `Principal`s with admin privileges over multiple `Tenant`s: ([#679](https://github.com/littlehorse-enterprises/littlehorse/pull/679))\\n\\nMulti-Tenancy and support for authentication + fine-grained ACL\'s via `Principal`s has been a labor of love implemented by [Eduwer Camacaro](https://github.com/eduwercamacaro), who has grown into the role of Grumpy Maintainer of LittleHorse.\\n\\n### Kafka Security Protocol Support\\n\\nPrior to release `0.8`, the LH Server could only access a Kafka cluster with either:\\n* Plaintext access with no security.\\n* TLS with no authentication.\\n* MTLS security.\\n\\nPR [#716](https://github.com/littlehorse-enterprises/littlehorse/pull/716) introduced the following Server configurations:\\n\\n* `LHS_KAFKA_SECURITY_PROTOCOL`\\n* `LHS_KAFKA_SASL_MECHANISM`\\n* `LHS_KAFKA_SASL_JAAS_CONFIG`\\n\\nThis allows for access to any Kafka cluster except those requiring loading custom implementations of callbacks on the client side (for example, using the Strimzi OAuth Plug-in).\\n\\nIt is now possible to run LH with Kafka as:\\n- No security (PLAINTEXT)\\n- TLS on the brokers, no authentication (SSL)\\n- MTLS on the brokers (SSL with TRUSTSTORE set)\\n- SASL with any JAAS config (SASL_SSL)\\n- Confluent Cloud.\\n\\n### LittleHorse Canary\\n\\nThe LittleHorse Canary was released in early access. Inspired by the [Strimzi Canary](https://strimzi.io/blog/2021/11/09/canary/) for Apache Kafka, the LittleHorse Canary is a system that runs workflows on LittleHorse and reports on the health of the cluster(s) that it is monitoring.\\n\\nThe LH Canary system comprises two components:\\n\\n1. The Metronome, which runs workflows and sends metric beats to a Kafka topic.\\n2. The Aggregator, which consumes the metrics beats Kafka topic and aggregates metrics to be exposed to Prometheus and a GRPC API.\\n\\nThe goal of the Canary is to monitor, profile, and benchmark LittleHorse Clusters from the same exact perspective as the clients who use them.\\n\\nThe Canary is the brain child of [Sa\xfal Pi\xf1a](https://github.com/sauljabin), who is also the author of the popular [Kaskade](https://github.com/sauljabin/kaskade) TUI for Apache Kafka.\\n\\n### Exponential Backoff Retry Policy\\n\\nPR ([#707](https://github.com/littlehorse-enterprises/littlehorse/pull/707)) introduced the ability to configure exponential backoff for `TaskRun` retries. Previously, only immediate retries were supported.\\n\\n### JavaScript Client\\n\\nWe published the first version of `littlehorse-client` on NPM [here](https://www.npmjs.com/package/littlehorse-client). This client contains the `LHConfig` in javascript, which provides access to our LittleHorse GRPC API. Note that we do not yet support a JavaScript Task Worker nor a JavaScript `WfSpec` SDK.\\n\\n### Bugfixes\\n\\nIn this release, we fixed several bugs:\\n* Task Worker improperly reported `EXCEPTION`s and `ERROR`s when throwing `LHTaskException` ([#738](https://github.com/littlehorse-enterprises/pull/738))\\n* Fixes task queue rehydration ([#727](https://github.com/littlehorse-enterprises/pull/727))\\n* Fixes the Retention Policy for `ExternalEventDef`\'s ([#724](https://github.com/littlehorse-enterprises/littlehorse/pull/724))\\n* Fixes deadlock in Java task worker ([#723](https://github.com/littlehorse-enterprises/littlehorse/pull/723))\\n* Fixes concurrency bug with the `AsyncWaiter` in the server ([#719](https://github.com/littlehorse-enterprises/littlehorse/pull/719))\\n* Fixes various issues from soak tests ([#706](https://github.com/littlehorse-enterprises/littlehorse/pull/706))\\n* Fixes to `NodeRun` lifecycle ([#665](https://github.com/littlehorse-enterprises/littlehorse/pull/665)).\\n\\n## Looking Forward\\n\\nWe continue to stabilize our API and add features that cover edge cases. Load testing, chaos testing, and soak testing are an ongoing project, and we are working with the Apache Kafka Community on a few bugfixes in the Kafka Streams library which is heavily used in the core of LittleHorse.\\n\\nOnce those action items are resolved, we will make a `1.0` release candidate. However, in the meantime we don\'t expect any massively-breaking API changes at the protocol level. However, certain syntactical changes may occur in our SDK\'s (especially Go and Python)."},{"id":"littlehorse-0.7-release","metadata":{"permalink":"/blog/littlehorse-0.7-release","source":"@site/blog/2024-01-28-0.7-release.md","title":"Releasing 0.7","description":"Approaching a stable `1.0.0` release.","date":"2024-01-28T00:00:00.000Z","tags":[{"inline":false,"label":"LittleHorse Orchestrator","permalink":"/blog/tags/littlehorse/","description":"Information about the LittleHorse Orchestrator."},{"inline":false,"label":"LittleHorse Releases","permalink":"/blog/tags/release/","description":"Release blogs for LittleHorse Orchestrator."}],"readingTime":4.535,"hasTruncateMarker":true,"authors":[{"name":"The LittleHorse Council","title":"The Council of LittleHorse Maintainers","description":"LittleHorse Orchestrator is maintained by LittleHorse Enterprises LLC and available under the SSPL license. The LittleHorse Council is the group of engineers inside LittleHorse Enterprises LLC who are responsible for the stewardship of the open-source Orchestrator project and charged with looking out for the best interests of the LH Community.","url":"https://littlehorse.io","page":{"permalink":"/blog/authors/lh-council"},"socials":{"github":"https://github.com/littlehorse-enterprises","linkedin":"https://www.linkedin.com/company/littlehorse"},"imageURL":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","key":"lh_council"}],"frontMatter":{"title":"Releasing 0.7","description":"Approaching a stable `1.0.0` release.","slug":"littlehorse-0.7-release","authors":["lh_council"],"tags":["littlehorse","release"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Releasing 0.8","permalink":"/blog/littlehorse-0.8-release"},"nextItem":{"title":"Releasing 0.5.0","permalink":"/blog/littlehorse-0.5.0-release"}},"content":"We are excited to announce the release of `0.7.2`! \x3c!-- truncate --\x3e This is our last release before we cut `1.0.0`, which will be the first stable and production-ready LittleHorse distribution.\\n\\n## Get Started\\n\\nLittleHorse is free for production use according to the Server-Side Public License!\\n\\nTo get started with LittleHorse OSS, you can:\\n\\n* Visit us on [GitHub](https://github.com/littlehorse-enterprises)\\n* Try our [quickstarts](https://littlehorse.dev/docs/developer-guide/install#installation-and-quickstart) or watch our founder, Colt, go through them in [Java](https://www.youtube.com/watch?v=8Zo_UOStg98&t=6s), [Go](https://www.youtube.com/watch?v=oZQc2ISSZsk), or [Python](https://www.youtube.com/watch?v=l3TZOjfpzTw)\\n* Join our [Slack Community](https://launchpass.com/littlehorse-community) for quick and responsive help!\\n\\nAlso, LittleHorse Enterprises LLC has released its first out our [product-focused website](https://littlehorse.io)! If you\'re still curious and want to learn even more, check out a few of our new in-depth tutorial series on [our YouTube page](https://www.youtube.com/@LittleHorse-ey3vw/featured).\\n\\n## New Features\\n\\nRelease `0.7` introduces many features designed to make your life easier. We plan to write blogs about all of them, so stay tuned!\\n\\n### Administrative Dashboard\\n\\nThe most exciting part of the `0.7.2` release of LittleHorse is the new LH Dashboard, which is an administrative portal into your LittleHorse Cluster. The LH Dashboard lets you check on all of your workflows and tasks and debug everything visually with fine-grained detail. Our quickstarts (see above) have everything you need to get started debugging your workflows with our dashboard.\\n\\nThe LH Dashboard is in the alpha stage, so we appreciate any bug reports or feature requests. Please file them on [our github](https://github.com/littlehorse-enterprises/littlehorse/issues)!\\n\\n### Idempotent Metadata Management\\n\\nManaging your `WfSpec`s and `TaskDef`s just got much easier. Check out our [updated docs](https://littlehorse.dev/docs/developer-guide/grpc/managing-metadata) for tutorials on how to keep your DevOps team happy and seamlessly integrate LittleHorse into your normal application development lifecycle.\\n\\n### Child Workflows\\n\\nWe also added the ability to run a `WfRun` which is a \\"child\\" of another `WfRun`. This allows for some interesting features, most importantly:\\n* Sharing `Variable`s between `WfRun`\'s\\n* Foreign-key relationships between the child and parent `WfRun`\'s.\\n\\nStay tuned for an upcoming blog about _why_ we added that feature. It was guided by our resident Domain-Driven Design expert, Eduwer Camacaro! Here\'s a hint: this feature makes it possible to use LittleHorse Workflows as a native data store for complex business entities. This is a great way to implement the \\"Aggregate Pattern.\\"\\n\\n### Enhanced `SearchWfRun`\\n\\nThe `rpc SearchWfRun` request now has a `repeated VariableMatch variable_filters` field on it. This allows you to filter `WfRun`\'s by the value of one or more `Variable`\'s when searching for them, returning only matching `WfRun`\'s. This is super useful when using a LittleHorse `WfRun` to model a business entity, and you need to do something like \\"find all orders placed by `user-id == john` and `status == OUT_FOR_SHIPPING`\\".\\n\\nIn the past, this was possible using the `rpc SearchVariable` and then back the `WfRunId` out of the `VariableId`; however, that method is a little bit clunky. In reality, our users want to find a `WfRunId` matching certain criteria; they\'re not looking for a `Variable`.\\n\\n## What\'s Next?\\n\\nWe couldn\'t be more excited about what is coming next.\\n\\n### Apache2 Clients\\n\\nSome members of the community have expressed concerns about our clients (SDK\'s + GRPC code) being licensed by the SSPL license. We heard you, and we will update them to the Apache 2.0 License before our `1.0.0` release! The server will remain SSPL.\\n\\n### Tutorials\\n\\nOne of our team members, Sohini, has been hard at work creating video tutorials which will help you get quickly up to speed on advanced LittleHorse concepts. You can find them here on our [YouTube](https://www.youtube.com/@LittleHorse-ey3vw/playlists).\\n\\nAdditionally, our founder has recorded a series of zoom meetings with himself (yes, you read that right...Colt used zoom to record a tutorial video series) going through quickstarts in all of our three SDK\'s. You can find them here in [Java](https://www.youtube.com/watch?v=8Zo_UOStg98&t=6s), [Go](https://www.youtube.com/watch?v=oZQc2ISSZsk), or [Python](https://www.youtube.com/watch?v=l3TZOjfpzTw).\\n\\n### Approaching `1.0.0`\\n\\nWhat\'s missing before `1.0.0`? We have some in-progress features that are already merged to `master` but only partially implemented. If you squint hard enough at our GRPC Api, you might notice that we have support for multi-tenancy and also fine-grained ACL\'s. They are NOT ready for production use as we need to iron out a few wrinkles, but we will have them ready for `1.0.0`. We also are working on an `rpc MigrateWfSpec` which allows you to migrate a running `WfRun` from an older version of a `WfSpec` to a newer version. This is hard work for us but it will be highly useful for our users.\\n\\nAdditionally, we are expanding our end-to-end test coverage to try to shake out as many issues as possible _before_ our users tell us about them. So far, the rate of new bugs that we\'ve discovered has slowed down considerably, which makes us think we are getting close to the quality we expect from our own product.\\n\\nWhat will change when we release `1.0.0`? We will be following [Semantic Versioning](https://semver.org) to the letter, which means we will be paying _super close attention_ to any breaking changes to our API. If we want our users to use us for mission critical workloads, we need to take stability seriously\u2014both in terms of performance and API compatibility.\\n\\nWe will also likely have three minor releases per year, with 12 months of patch support for each minor release. This release schedule is copied from Apache Kafka.\\n\\n### LH Cloud\\n\\nLastly, stay tuned for LittleHorse Cloud! Early access is open. If you would like to sign up for early access to LH Cloud, visit [our website](https://www.littlehorse.io/lh-cloud) or contact `sales@littlehorse.io`."},{"id":"littlehorse-0.5.0-release","metadata":{"permalink":"/blog/littlehorse-0.5.0-release","source":"@site/blog/2023-09-08-0.5.0-release.md","title":"Releasing 0.5.0","description":"Python, For-Each, LH Platform.","date":"2023-09-08T00:00:00.000Z","tags":[{"inline":false,"label":"LittleHorse Orchestrator","permalink":"/blog/tags/littlehorse/","description":"Information about the LittleHorse Orchestrator."},{"inline":false,"label":"LittleHorse Releases","permalink":"/blog/tags/release/","description":"Release blogs for LittleHorse Orchestrator."}],"readingTime":5.205,"hasTruncateMarker":true,"authors":[{"name":"The LittleHorse Council","title":"The Council of LittleHorse Maintainers","description":"LittleHorse Orchestrator is maintained by LittleHorse Enterprises LLC and available under the SSPL license. The LittleHorse Council is the group of engineers inside LittleHorse Enterprises LLC who are responsible for the stewardship of the open-source Orchestrator project and charged with looking out for the best interests of the LH Community.","url":"https://littlehorse.io","page":{"permalink":"/blog/authors/lh-council"},"socials":{"github":"https://github.com/littlehorse-enterprises","linkedin":"https://www.linkedin.com/company/littlehorse"},"imageURL":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","key":"lh_council"}],"frontMatter":{"title":"Releasing 0.5.0","description":"Python, For-Each, LH Platform.","slug":"littlehorse-0.5.0-release","authors":["lh_council"],"tags":["littlehorse","release"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Releasing 0.7","permalink":"/blog/littlehorse-0.7-release"},"nextItem":{"title":"Helm and Kubernetes Operators","permalink":"/blog/helm-and-k8s-operators"}},"content":"We are excited to announce the minor release `0.5.0`. \x3c!-- truncate --\x3e This release is highlighted by:\\n\\n* Alpha support for building `WfSpec`s in Python.\\n* Improved monitoring and health metrics on the LittleHorse Server.\\n* Support for looping over a `JSON_ARR` and launching threads in parallel for each element.\\n* Improved Exception Handling.\\n* Limited early access for LittleHorse Platform.\\n\\nIn this release, we made great strides towards full Python support, improved monitoring and observability, and added the ability to spawn threads in parallel looping over a `JSON_ARR` variable.\\n\\n## Get Started\\n\\nLittleHorse is free for production use according to the Server-Side Public License!\\n\\nTo get started with LittleHorse OSS, you can:\\n\\n* Try our [quickstarts](https://littlehorse.dev/docs/developer-guide/install)\\n* Visit us on [GitHub](https://github.com/littlehorse-enterprises/littlehorse) and give us a :star:!\\n* Download our [docker images](https://gallery.ecr.aws/littlehorse)\\n\\n## New Features\\n\\nWe\'d like to highlight some of the exciting new features in `0.5.0`.\\n\\n### Python `WfSpec` Support\\n\\nOur Python SDK now has full support for building `WfSpec`s! You can check it out at our [quickstart page](/docs/developer-guide/install).\\n\\n### For-Each Suppport\\n\\nThis is a very exciting feature which allows you to iterate over a list and spawn multiple `ThreadRun`s (like threads in a program).\\n\\nTo see it in action, check out our [example](https://github.com/littlehorse-enterprises/littlehorse/tree/master/examples/spawn-thread-foreach) or read the [documentation](https://littlehorse.dev/docs/developer-guide/wfspec-development/child-threads).\\n\\n### Improved Failure Handling\\n\\nThis release introduces a new status for LittleHorse, called `EXCEPTION`. The `EXCEPTION` status differs from the `ERROR` status in the following ways:\\n\\n* `ERROR` means an unexpected _technical_ failure occurred. For example, a `TaskRun` timed out because a third-party API was down.\\n* `EXCEPTION` means that a failure occurred at the _business process level_. For example, you might use an `EXCEPTION` when a customer has insufficient funds in her account to complete an order.\\n\\nJust like in programming, you can throw and catch `EXCEPTION`s (and you can also catch `ERROR`s). For a blog post that goes in-depth into how LittleHorse makes it easy to handle failures in your workflows, check out our [Failure Handling Docs](/docs/concepts/workflows#failure-handling).\\n\\n### LH Server Monitoring\\n\\nWe added a new path `/status` on the LH Server\'s health endpoint (port `1822` by default) which can be used to inspect the status of all internal Kafka Streams `Task`s on the LH Server. It presents the following information:\\n\\n* All Active Tasks on the host\\n* All Standby Tasks on the host\\n* Any ongoing State Restorations on the host\\n\\nAdditionally, we added a `/diskUsage` endpoint which returns the number of bytes of disk space in use by the LH Server.\\n\\nLittleHorse Platform uses these endpoints to intelligently scale, manage, and operate LittleHorse for you.\\n\\nWe are also in the process of writing and implementing a Kafka Improvement Proposal to improve visibility of Standby Tasks, which will allow the LittleHorse Operator (both in LH Platform and LH Cloud) to safely and smoothly scale LittleHorse clusters down without any downtime. Stay tuned in the Kafka developer mailing list!\\n\\n### LH Platform\\n\\nLittleHorse Platform is a Kubernetes Operator that securely manages a LittleHorse cluster for you in your own environment. It seamlessly integrates with your Kubernetes environment, GitOps workflows, and security strategy (TLS, mTLS, OAuth, Cert Manager, Keycloak).\\n\\nLittleHorse Platform is now available for limited early access, and has been installed in one of the largest health insurance companies in the US.\\n\\nTo get started with LittleHorse Platform, please [contact us](https://docs.google.com/forms/d/e/1FAIpQLScXVvTYy4LQnYoFoRKRQ7ppuxe0KgncsDukvm96qKN0pU5TnQ/viewform?usp=sf_link).\\n\\n### Persistent Variables\\n\\nIn LittleHorse `0.2.0` and later, you can search for `Variable`s by their value. For example, if you have a Workflow Specification that defines a variable `email_address`, you can find all Workflow Run\'s where `email_address == \'obiwan@jedi-council.org`  by using the `SearchVariable` rpc call.\\n\\nThe problem with `0.2.0`? You need to provide the `wfSpecVersion` in your search request. That means you can only search for a `Variable` if you know the version of the `WfSpec` it came from.\\n\\nRelease `0.4.0` introduced the ability to mark a `Variable` as `persistent`, which means that:\\n* Every future version of the `WfSpec` must have the same variable definition with the same index type.\\n* You can now search for variables with a certain value across _all versions_ of the `WfSpec`.\\n\\nBe on the lookout for an upcoming blog post about using Persistent Variables and a simple backend-for-frontend to build an end-to-end Approval Workflow Application using only LittleHorse!\\n\\n## What\'s Next\\n\\nOver the next few weeks, we plan to:\\n\\n* Add utilities to make it easier to work with the LittleHorse API.\\n* Allow users to throw a Workflow `EXCEPTION` from within the Task Worker SDK (currently, only `ERROR` is supported).\\n* Continue hardening the LittleHorse Server\'s availability and performance story.\\n* Launch limited early accesss for LittleHorse Cloud and LittleHorse UI.\\n\\nTo get started with LittleHorse, head over to our [installation docs](https://littlehorse.dev/docs/developer-guide/install).\\n\\n### What about `0.3.0` and `0.4.0`?\\n\\nWe also released `0.3.0` and `0.4.0` over the past 5 weeks! (And before `0.3.0`, we had a minor patch bugfix on `0.2.1`).\\n\\nThe only thing missing with `0.3.0` and `0.4.0` is a blog post + announcement. That\'s because a lot of the features we included in this announcement were partially-implemented, implemented in some languages and not others, or in the \\"experimental\\" phase at the time of `0.3.0` and `0.4.0`. We accelerated the release of `0.3.0` and `0.4.0` because certain early-access customers requested certain features on an accelerated timeline.\\n\\nAs our API is mostly stable now, we will slow down our release cadence to likely a new `*.x.*` version (a `minor` release in [Semantic Versioning](https://semver.org)) every two months, with security and bugfix patch releases (`*.*.x`) as needed.\\n\\nAdditionally, as we introduce new features, we will start a release changelog document in which we document the level of stability of the new API\'s introduced. For example:\\n* `STABLE`: Any changes to this API before the next [Major Release](https://semver.org) will be backwards compatible. The feature is covered by our integration tests.\\n* `BETA`: We don\'t anticipate any _large breaking changes_ to the feature/API. It is covered by our integration tests, but it _might_ change before the `1.0.0` release.\\n* `EXPERIMENTAL`: Try it out and give us feedback! But you might want to wait a release or two before putting it into production.\\n\\nThe `0.6.0` release notes will include a table of all of our features and their API Stability Level in all four of our SDK\'s."},{"id":"helm-and-k8s-operators","metadata":{"permalink":"/blog/helm-and-k8s-operators","source":"@site/blog/2023-09-01-helm-and-k8s-operators.md","title":"Helm and Kubernetes Operators","description":"To Helm or to Operator?","date":"2023-09-01T00:00:00.000Z","tags":[{"inline":false,"label":"Technical Analysis","permalink":"/blog/tags/analysis/","description":"Analysis of the current and future state of Technical Architecture."}],"readingTime":5.58,"hasTruncateMarker":true,"authors":[{"name":"Colt McNealy","title":"Managing Member of the LLC","description":"Colt is the founder of LittleHorse Enterprises and the original author of the LittleHorse Orchestrator. He\'s a passionate Apache Kafka fan and loves hockey, golf, piano, cooking, and Taekwondo.","page":{"permalink":"/blog/authors/coltmcnealy"},"socials":{"github":"https://github.com/coltmcnealy-lh","linkedin":"https://www.linkedin.com/in/colt-mcnealy-900b7a148/","x":"https://x.com/coltmcnealy"},"imageURL":"https://avatars.githubusercontent.com/u/100447728","key":"coltmcnealy"}],"frontMatter":{"title":"Helm and Kubernetes Operators","description":"To Helm or to Operator?","slug":"helm-and-k8s-operators","authors":["coltmcnealy"],"tags":["analysis"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Releasing 0.5.0","permalink":"/blog/littlehorse-0.5.0-release"},"nextItem":{"title":"Releasing 0.2.0","permalink":"/blog/littlehorse-0.2.0-release"}},"content":"About [Helm](https://helm.sh) vs Kubernetes Operators.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Helm\\n\\nHelm is like `brew` or `npm` for Kubernetes. There are repositories containing charts, and each chart allows you to install an application into your K8s cluster.\\n\\n### How it Works\\n\\nUnder the hood, Helm works by filling out some templated Kubernetes yaml files with user-provided values, drastically reducing boilerplate and allowing you to deploy a reasonably complex application without the user of the helm chart having to understand too much about how to manage such an application.\\n\\nIn addition to that template-engine functionality, Helm also manages versions of your application. You can use Helm to release a new version of your app (for example, updating the docker image tag) and then quickly roll back to a previous version if you discover a bug. This is great for teams deploying stateless applications such as microservices or [LittleHorse Task Workers](https://littlehorse.dev/docs/concepts/task-workers).\\n\\n### The Good\\n\\nFirst, it is quite simple to write a Helm chart. This means that most DevOps teams can quickly write a helm chart that can be used by microservice teams across their organization.\\n\\nSecondly, Helm is a client library (well, it has been since the removal of Tiller...but that\'s another blog post). Therefore, you don\'t need to run any privileged pods inside the K8s cluster; all you need is a CI server with permission to create the necessary K8s resources.\\n\\n### Limitations\\n\\nUnfortunately, Helm doesn\'t do much beyond initial installation and upgrades. Monitoring, self-healing, autoscaling, certificate rotation, and management of non-Kubernetes resources (eg. Kafka Topics, LittleHorse Task Definitions, AWS LoadBalancers, etc) are some exercises left to the reader, to name just a few.\\n\\n## Operators\\n\\n[Kubernetes Operators](https://operatorframework.io/) are a pattern introduced by Red Hat that intends to capture the knowledge of an expert Site Reliability Engineer (or, more punnily, a software operator) into a program that manages (or operates) a complex application.\\n\\nTo accomplish this, a Kubernetes Operator _extends_ the Kubernetes API to introduce a new resource type that is custom-made for your application. The Operator works in tandem with Kubernetes itself to manage applications of a specific type.\\n\\n### How they Work\\n\\nA Kubernetes Operator has two components:\\n\\n1. A `CustomResourceDefinition`, which defines the extension to the Kubernetes API (including relevant configurations for your application type).\\n2. A Controller, which watches any resources from your Custom Resource Definition and \\"reconciles\\" them.\\n\\nThe `CustomResourceDefinition` can be over-simplified as an Open API (not Open AI) specification for how your custom resource will look. For example, in LittleHorse Platform, the simplest version of a `LHCluster` resource (which creates a, you guessed it, LittleHorse Cluster) is:\\n\\n```yaml\\napiVersion: littlehorse.io/v1\\nkind: LHCluster\\nmetadata:\\n  name: hello-littlehorse\\n  namespace: lh\\nspec:\\n  server:\\n    version: \\"0.2.0\\"\\n    listeners:\\n    - name: internal-k8s\\n      type: internal\\n      port: 2023\\n    replicas: 3\\n    storage:\\n      volumeSize: \\"10G\\"\\n  kafka:\\n    strimziClusterRef:\\n      clusterName: my-strimzi-kafka-cluster\\n      listenerPort: 9093\\n```\\n\\nThe `CustomResourceDefinition` allows you to `kubectl apply -f <that file up there>`, and then you can `kubectl get lhclusters`:\\n\\nNow how does the LittleHorse cluster get created, configured, managed, and monitored? That\'s where the Controller comes into play. In the Operator pattern, a Controller is a process (normally, it runs as a `Pod` in a cluster) that watches for all events related to a `CustomResourceDefinition` and manipulates the external world to match what the Custom Resources specify.\\n\\nGenerally, that means creating a bunch of Kubernetes `Service`s, `Deployment`s, etc. to spin up an instance of an application. For example, the [Strimzi](https://strimzi.io) Kafka Operator watches `Kafka` resources and deploys an actual Kafka cluster.\\n\\nHowever, a Controller can also manage non-kuberentes resources. For example, many `Ingress` controllers provision or configure physical load balancers outside of the Kubernetes cluster. As another great example, the [Strimzi](https://strimzi.io) Kafka Topic Operator watches for `KafkaTopic` resources and creates (you guessed it) Kafka Topics using the Kafa Admin API.\\n\\nWe at LittleHorse plan to add similar CRD\'s that are specific to LittleHorse...stay tuned to learn about the `LHTaskDef` and `LHPrincipal` CRD\'s :wink:.\\n\\n### The Good\\n\\nKubernetes Operators are beautiful. Since they were developed by Red Hat, they (along with [Strimzi](https://strimzi.io)) are the biggest reason why Red Hat is in my top-three favorite software companies of all time.\\n\\nA well-written operator can make it a breeze to manage even the most daunting applications. Since the Controller is code written in a general-purpose language (normally Go or Java), an Operator can do just about anything that can be automated by an SRE. This includes:\\n* Autoscaling and alerting based on metrics\\n* Self-healing and mitigation in the face of hardware faults or degradations\\n* Certificate rotation\\n* Metadata management in your application (for example, creating Kafka Users)\\n* Intelligent rolling restarts that preserve high availability\\n* Provisioning infrastructure _outside of_ Kubernetes, for example [CrossPlane](https://crossplane.io).\\n\\n### The Ugly\\n\\nThe biggest downside to Operators in Kubernetes is that writing a Controller is _hard_. Additionally, it requires running a `Pod` with special privileges that allow the `Pod` to create other K8s resources. Because of this, writing an Operator for something like standardizing your team\'s blueprints for deploying a microservice just doesn\'t make sense.\\n\\nFuture blogs will dive into some of the challenges that we had to overcome with LittleHorse Platform, and how we minimized the permissions that our Operator needs to provide a self-driving LittleHorse experience to our customers.\\n\\n## Helm or Operators?\\n\\nWell, I\'m a software engineer, so I\'m going to say \\"it depends.\\" However, Kafka legend Gwen Shapira said in a fantastic [podcast](https://open.spotify.com/episode/0BYwF3e8y5OzrPt0xYMyqb?si=0c7d44154b434d0e) that some \\"it depends\\" are more helpful than others. So in an effort to fall in the \\"more helpful\\" side:\\n\\n* If you want a framework for deploying simple stateless applications while minimizing boilerplate (i.e. allowing different teams to deploy microservices), then you probably want Helm.\\n* If your application doesn\'t require much hand-holding after initial configuration on Kubernetes, Helm might do.\\n* If you want to provide a Kubernetes-native way to manage non-kubernetes infrastructure, you need an Operator.\\n* If you want to provide a self-driving experience for consumers of a highly complex application such as Kafka, ElasticSearch, or LittleHorse, you need an Operator.\\n\\n### LittleHorse Platform\\n\\nLittleHorse Platform is an enterprise-ready distribution of LittleHorse that runs in your own Kubernetes environment. We believe that Helm is fantastic for deploying many stateless applications, and even some stateful applications. However, Helm wouldn\'t let us go far enough towards providing our customers with a fully self-driving LittleHorse experience. As such, we chose to put in the extra work and build a full Kubernetes Operator. Stay tuned for an extensive list of current and upcoming LH Platform features, all powered by the [Java Operator SDK](https://javaoperatorsdk.io).\\n\\nTo inquire about LittleHorse Platform, contact `sales@littlehorse.io`. To get started with LittleHorse Community (free for production use under the SSPL), check out our [Installation Docs](https://littlehorse.dev/docs/developer-guide/install)."},{"id":"littlehorse-0.2.0-release","metadata":{"permalink":"/blog/littlehorse-0.2.0-release","source":"@site/blog/2023-08-30-0.2.0-release.md","title":"Releasing 0.2.0","description":"Making workflow development easy again.","date":"2023-08-30T00:00:00.000Z","tags":[{"inline":false,"label":"LittleHorse Orchestrator","permalink":"/blog/tags/littlehorse/","description":"Information about the LittleHorse Orchestrator."},{"inline":false,"label":"LittleHorse Releases","permalink":"/blog/tags/release/","description":"Release blogs for LittleHorse Orchestrator."}],"readingTime":3.54,"hasTruncateMarker":true,"authors":[{"name":"The LittleHorse Council","title":"The Council of LittleHorse Maintainers","description":"LittleHorse Orchestrator is maintained by LittleHorse Enterprises LLC and available under the SSPL license. The LittleHorse Council is the group of engineers inside LittleHorse Enterprises LLC who are responsible for the stewardship of the open-source Orchestrator project and charged with looking out for the best interests of the LH Community.","url":"https://littlehorse.io","page":{"permalink":"/blog/authors/lh-council"},"socials":{"github":"https://github.com/littlehorse-enterprises","linkedin":"https://www.linkedin.com/company/littlehorse"},"imageURL":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","key":"lh_council"}],"frontMatter":{"title":"Releasing 0.2.0","description":"Making workflow development easy again.","slug":"littlehorse-0.2.0-release","authors":["lh_council"],"tags":["littlehorse","release"],"image":"https://avatars.githubusercontent.com/u/140006313?s=400&u=7bf4c91d92dfe590ac71bb6b4821e1a81aa5b712&v=4","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Helm and Kubernetes Operators","permalink":"/blog/helm-and-k8s-operators"}},"content":"We are excited to announce the release of `0.2.0`! \x3c!-- truncate --\x3e In this release, we added several new features, highlighted by User Tasks, security, and Python support.\\n\\n## Get Started\\n\\nLittleHorse is free for production use according to the Server-Side Public License!\\n\\nTo get started with LittleHorse OSS, you can:\\n\\n* Visit us on [GitHub](https://github.com/littlehorse-enterprises)\\n* Try our [quickstarts](https://littlehorse.dev/docs/developer-guide/install#installation-and-quickstart)\\n\\nAdditionally, with version `0.2.0`, we have released our first two Docker Images:\\n\\n* [`lh-server`](https://gallery.ecr.aws/littlehorse/littlehorse-server), the production-ready build of the LittleHorse Server.\\n* [`lh-standalone`](https://gallery.ecr.aws/littlehorse/littlehorse-standalone), a self-contained build of the LittleHorse Server that you can run to get a working LH Installation for local development.\\n\\n## New Features\\n\\nRelease `0.2.0` contains many exciting new features, and we\'ve highlighted a few here.\\n\\n### User Tasks\\n\\n[User Tasks](https://littlehorse.dev/docs/concepts/user-tasks) are a massive new feature released in `0.2.0` which allow you to schedule tasks to be executed by a human user alongside tasks that are executed by computers.\\n\\nIn `0.2.0`, User Tasks have reached stability, meaning that future releases will be backwards-compatible with the current User Tasks API. We currently have the following features:\\n\\n* Assignment of tasks to a User or User Group\\n* Reminder Tasks, or `TaskRun`\'s that are scheduled some time after a `UserTaskRun` is scheduled.\\n* Automatic reassignment of a `UserTaskRun` after some period of inactivity.\\n* Manual reassignment of a `UserTaskRun`.\\n* `UserTaskRun` search.\\n\\n:::note\\nThe public API for User Tasks is stable in all of the grpc clients and in the Java `WfSpec` SDK.\\n\\nThe Go and Python grpc clients both support User Tasks. However, neither Python nor Go yet have support for User Tasks in the `WfSpec` SDK.\\n:::\\n\\n### Workflow Threading\\n\\nRelease `0.2.0` allows you to use a `WAIT_FOR_THREADS` node to wait for more than one child thread at one time. For an example, see our [Parallel Approval Example](https://github.com/littlehorse-enterprises/littlehorse/tree/master/examples/parallel-approval) on our GitHub.\\n\\nFuture releases will provide _backwards-compatible_ enhancements to this\\nfunctionality, allowing various strategies for handling failures of individual child threads.\\n\\n### Python Support\\n\\nWe have released an alpha [Python SDK](https://github.com/littlehorse-enterprises/littlehorse/tree/master/sdk-python)! This release contains:\\n\\n* Python client in grpc\\n* Python Task Worker SDK\\n\\nCurrently, building `WfSpec`\'s in Python is not supported. We aim to move python Task Worker support from alpha to beta, and add alpha support for `WfSpec` development in python, in the `0.3.0` release.\\n\\nTo try out our python task worker client, you can head to [Installation Docs](https://littlehorse.dev/docs/developer-guide/install) and the [Task Worker Development Docs](https://littlehorse.dev/docs/developer-guide/task-worker-development).\\n\\n:::note\\nThe Python SDK is in the alpha stage, meaning that future releases could break backwards compatibility.\\n:::\\n\\n### Security\\n\\nWe added beta support for OAuth, TLS, and mTLS in release `0.2.0`. The following features graduated to \\"beta\\" in this release:\\n\\n* TLS encryption for incoming connections on all listeners, configured on a per-listener basis.\\n* mTLS to authenticate incoming connections on any listeners, configured on a per-listener basis.\\n* OAuth to authenticate incoming connections on any public listener (excluding the inter-server communication port).\\n\\n:::info\\nBeta support means that we will soon add significant functionality, and as such a future release _might_ break backwards compatibility.\\n\\nHowever, future releases of a feature in the _beta_ state will most likely be backwards compatible with `0.2.0` barring exceptional circumstances.\\n:::\\n\\n### Performance\\n\\nWe made several optimizations to our storage management sub-system, reducing the number of put\'s and get\'s into our backing state store by roughly 30%. As a result, a LittleHorse Server running with a single partition is capable of scheduling over 1,100 `TaskRun`\'s per second.\\n\\n### Go Support\\n\\nSupport for the Go client is now beta. Future releases will maintain compatibility for all features on our documentation.\\n\\nRelease `0.3.0` will close the gap between the Java and Go SDK\'s, adding features such as:\\n* Format Strings for Variable Assignments in the `WfSpec` SDK\\n* User Task support in the `WfSpec` SDK\\n* Configuring Indexes on `Variable`s in the `WfSpec` SDK\\n\\n## What\'s Next\\n\\nWe have several exciting features coming soon over the next few releases, including:\\n\\n* Fine-grained access controls\\n* Backward-compatible improvements to [Failure Handling](https://littlehorse.dev/docs/concepts/exception-handling)\\n* C# support\\n* Python support for building `WfSpec`s\\n\\nFor an enterprise-ready distribution of LittleHorse running in your own datacenter, contact `sales@littlehorse.io` to inquire about LittleHorse Platform.\\n\\nFor a pay-as-you-go, serverless Managed Service of LittleHorse in the cloud, fill out the [LH Cloud Waitlist Form](https://docs.google.com/forms/d/e/1FAIpQLScXVvTYy4LQnYoFoRKRQ7ppuxe0KgncsDukvm96qKN0pU5TnQ/viewform)."}]}}')}}]);