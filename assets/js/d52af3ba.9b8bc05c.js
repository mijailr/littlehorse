"use strict";(self.webpackChunklh_site=self.webpackChunklh_site||[]).push([[9403],{8682:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var s=r(4848),t=r(8453);const i={},o="Operations Overview",a={id:"operations/overview",title:"Operations Overview",description:"A minimal LittleHorse cluster has the following components:",source:"@site/docs/06-operations/00-overview.md",sourceDirName:"06-operations",slug:"/operations/overview",permalink:"/docs/operations/overview",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:0,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Operations",permalink:"/docs/operations/"},next:{title:"Server Configurations",permalink:"/docs/operations/server-configuration"}},l={},c=[{value:"LH Server",id:"lh-server",level:2},{value:"Persistent Storage",id:"persistent-storage",level:3},{value:"Advertised Listeners",id:"advertised-listeners",level:3},{value:"Internal Listeners",id:"internal-listeners",level:3},{value:"Kafka Streams",id:"kafka-streams",level:3},{value:"Kafka",id:"kafka",level:2},{value:"Topics",id:"topics",level:3},{value:"Security",id:"security",level:3},{value:"Workload",id:"workload",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"operations-overview",children:"Operations Overview"}),"\n",(0,s.jsx)(n.p,{children:"A minimal LittleHorse cluster has the following components:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["LH Server (eg. the ",(0,s.jsx)(n.a,{href:"https://gallery.ecr.aws/littlehorse/lh-server",children:(0,s.jsx)(n.code,{children:"lh-server"})})," docker image)"]}),"\n",(0,s.jsx)(n.li,{children:"A Kafka Cluster"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"With just this setup, you can run a working LittleHorse cluster, potentially relying on mTLS authentication for security. If you wish to delegate authentication to an OAuth Server, we introduce an additional architectural component:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"OAuth Identity Provider"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The IdP can be used to authenticate machine clients as well as human clients (eg. a human using ",(0,s.jsx)(n.code,{children:"lhctl"})," or the LH Dashboard). Speaking of the Dashboard, users who wish to run LittleHorse can also introduce:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The LH Dashboard, usually via the ",(0,s.jsx)(n.a,{href:"https://gallery.ecr.aws/littlehorse/lh-dashboard",children:(0,s.jsx)(n.code,{children:"lh-dashboard"})})," dockerimage"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Lastly, a production-ready deployment of any technology requires monitoring. The LittleHorse Server exposes prometheus-compatible metrics (by default, on the port ",(0,s.jsx)(n.code,{children:"1822"})," in honor of the year of Ecuador's independence). As such, you might introduce one final component:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A prometheus-compatible monitoring system."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"lh-server",children:"LH Server"}),"\n",(0,s.jsxs)(n.p,{children:["The LH Server internally is a stateful Kafka Streams application, and it exposes a public ",(0,s.jsx)(n.a,{href:"/docs/developer-guide/grpc",children:"GRPC API"}),". Note that Kafka is an implementation detail of LittleHorse and, as of ",(0,s.jsx)(n.code,{children:"0.7.0"}),", is not exposed to the LittleHorse clients."]}),"\n",(0,s.jsxs)(n.p,{children:["At its core, the LH Server is itself just a Java application that stores information locally on disk and talks to a Kafka cluster. Therefore, all you ",(0,s.jsx)(n.em,{children:"really"})," need is a disk, a Kafka Cluster, and a JVM."]}),"\n",(0,s.jsx)(n.h3,{id:"persistent-storage",children:"Persistent Storage"}),"\n",(0,s.jsx)(n.p,{children:"The LittleHorse Server processes all data and serves all queries from RocksDB. RocksDB stores data in SST files on disk. The LittleHorse Server uses disk to persist the data stored on RocksDB between server restarts (i.e. during a rolling upgrade or after a crash recovery). If an LH Server instance starts up and data is missing, then the data on RocksDB is re-constructed by replaying Kafka changelog topics. This process is time-consuming but it does ensure reliability so long as your Kafka cluster is durable. However, this process can largely be avoided by providing persistent storage to the LH Server."}),"\n",(0,s.jsxs)(n.p,{children:["The most important takeaway from this section is that ",(0,s.jsx)(n.strong,{children:"the LH Server is stateful"}),", so you should provision sufficient storage to handle your workloads, and also ensure that you monitor free disk space."]}),"\n",(0,s.jsx)(n.h3,{id:"advertised-listeners",children:"Advertised Listeners"}),"\n",(0,s.jsxs)(n.p,{children:["For most uses of the ",(0,s.jsx)(n.a,{href:"/docs/developer-guide/grpc",children:"GRPC API"}),", the client can connect to any LH Server in the cluster, and the contacted server will transparently route the request to the appropriate other LH Server Instance(s), and return the final result back to the client. For this use-case, the clients do not need the ability to connect to a specific LH Server Instance."]}),"\n",(0,s.jsxs)(n.p,{children:["However, the Task Workers need to be able to address individual servers directly. This is because, to avoid costly distributed coordination, a scheduled ",(0,s.jsx)(n.code,{children:"TaskRun"})," is only managed and maintained by a single LH Server, and the internal Task Queue's are partitioned by the server. Therefore, in order to ensure that all ",(0,s.jsx)(n.code,{children:"TaskRun"}),"'s are completed, the Task Workers for a certain ",(0,s.jsx)(n.code,{children:"TaskDef"})," collectively need to connect to all of the LH Server Instances."]}),"\n",(0,s.jsx)(n.p,{children:"In order to reduce wasteful network connections, the LH Server has a Task Worker Assignment Protocol which, upon every Task Worker Heartbeat, assigns a list of LH Server Instances for each Task Worker to connect to."}),"\n",(0,s.jsx)(n.p,{children:'As a consequence, every LH Server needs to have an "advertised" host and port for each configured internal listener, so that the Task Workers can "discover" where to connect to.'}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"This all sounds really complicated, but don't worry! It happens transparently under the hood in our Task Worker SDK. You won't have to worry about balancing Task Workers; all you need to do is configure advertised listeners! This is similar to Kafka Consumer Groups."})}),"\n",(0,s.jsx)(n.h3,{id:"internal-listeners",children:"Internal Listeners"}),"\n",(0,s.jsx)(n.p,{children:"LittleHorse is a partitioned system, meaning that not all data lives on all of the nodes. Therefore, when a request arrives on Server Instance 1, instance 1 may have to ask Instance 2 for the answer! LittleHorse has a special port for LH Server Instances to communicate with each other."}),"\n",(0,s.jsx)(n.h3,{id:"kafka-streams",children:"Kafka Streams"}),"\n",(0,s.jsx)(n.p,{children:"LittleHorse is built on Kafka Streams because, quite simply, there was no other way to reach the performance numbers we wanted with any other backing data store (note: benchmarks are coming soon!)."}),"\n",(0,s.jsx)(n.p,{children:"It's safe to say that Kafka Streams is an incredibly powerful and beautiful piece of technology. However, with great power comes great complexity, so it's advisable that you understand Streams at a basic level before running the LittleHorse Server in production."}),"\n",(0,s.jsxs)(n.p,{children:["For some primers on Kafka Streams operations, our friends at ",(0,s.jsx)(n.a,{href:"https://responsive.dev",children:"Responsive"})," have posted some fantastic ",(0,s.jsx)(n.a,{href:"https://responsive.dev/blog",children:"Blog Posts"})," that you should check out. These blogs are general to Kafka Streams, not LittleHorse, but we have considered those topics when running LittleHorse in production for LittleHorse Cloud."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["If you are concerned about the complexity of running LittleHorse in production, don't worry! You can also use our ",(0,s.jsx)(n.a,{href:"https://littlehorse.io/lh-cloud",children:"Cloud Service"}),", or get expert support from LittleHorse Enterprises when running on-premise."]})}),"\n",(0,s.jsx)(n.h2,{id:"kafka",children:"Kafka"}),"\n",(0,s.jsx)(n.p,{children:"Properly configuring Kafka is necessary for a production-ready LittleHorse installation."}),"\n",(0,s.jsx)(n.h3,{id:"topics",children:"Topics"}),"\n",(0,s.jsxs)(n.p,{children:["LittleHorse is internally a Kafka Streams application with ",(0,s.jsx)(n.a,{href:"/docs/architecture-and-guarantees#kafka-streams-topologies",children:"four sub-topologies"}),". These topologies require having proper Kafka Topics configured. The required topics are:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Core Command Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-core-cmd"'})}),"\n",(0,s.jsxs)(n.li,{children:["Partition Count: ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_PARTITIONS"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Core Changelog Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-core-store-changelog"'})}),"\n",(0,s.jsxs)(n.li,{children:["Partition Count: ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_PARTITIONS"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Repartition Command Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-core-repartition"'})}),"\n",(0,s.jsxs)(n.li,{children:["Partition Count: ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_PARTITIONS"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Repartition Changelog Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-core-repartition-store-changelog"'})}),"\n",(0,s.jsxs)(n.li,{children:["Partition Count: ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_PARTITIONS"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Metadata Command Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-global-metadata-cl"'})}),"\n",(0,s.jsx)(n.li,{children:"Partition Count: 1"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Metadata Changelog Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-global-metadata-cl"'})}),"\n",(0,s.jsx)(n.li,{children:"Partition Count: 1"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Timer Command Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-timers"'})}),"\n",(0,s.jsxs)(n.li,{children:["Partition Count: ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_PARTITIONS"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Timer Changelog Topic","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"{LHS_CLUSTER_ID}-timer-changelog"'})}),"\n",(0,s.jsxs)(n.li,{children:["Partition Count: ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_PARTITIONS"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"security",children:"Security"}),"\n",(0,s.jsx)(n.p,{children:"We recommend that you create a Kafka Principal for the LH Server. It requires the following permissions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Topics:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"DESCRIBE"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"DESCRIBECONFIGS"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"IDEMPOTENTWRITE"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"WRITE"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"READ"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"CLUSTERACTION"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Groups:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ALL"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Transaction ID:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"ALL"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For security, all rules should be scoped to ",(0,s.jsx)(n.strong,{children:"only"})," entities with a prefix matching the ",(0,s.jsx)(n.code,{children:"LHS_CLUSTER_ID"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"workload",children:"Workload"}),"\n",(0,s.jsx)(n.p,{children:"It should be noted that the LittleHorse workload heavily uses Kafka Transactions and compacted topics. In particular, the transaction-heavy nature of the workload means that, relative to other Kafka workloads, the brokers used by LittleHorse will require a higher ratio of CPU to Network Bandwidth."}),"\n",(0,s.jsx)(n.p,{children:"As with all Kafka deployments, it is strongly recommended to provision significant memory for your Kafka brokers so that tail-reading consumers (i.e. the LH Server) can read data fresh off the Kafka Broker's page cache rather than reading from disk. This has a significant effect on the latency of the LH Server."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);